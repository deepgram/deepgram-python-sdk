# This file was auto-generated by Fern from our API Definition.

import typing

from ....core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ....core.request_options import RequestOptions
from .raw_client import AsyncRawMediaClient, RawMediaClient
from .types.media_transcribe_request_callback_method import MediaTranscribeRequestCallbackMethod
from .types.media_transcribe_request_custom_intent_mode import MediaTranscribeRequestCustomIntentMode
from .types.media_transcribe_request_custom_topic_mode import MediaTranscribeRequestCustomTopicMode
from .types.media_transcribe_request_encoding import MediaTranscribeRequestEncoding
from .types.media_transcribe_request_model import MediaTranscribeRequestModel
from .types.media_transcribe_request_summarize import MediaTranscribeRequestSummarize
from .types.media_transcribe_request_version import MediaTranscribeRequestVersion
from .types.media_transcribe_response import MediaTranscribeResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class MediaClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawMediaClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawMediaClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawMediaClient
        """
        return self._raw_client

    def transcribe_url(
        self,
        *,
        url: str,
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> MediaTranscribeResponse:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        url : str

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        MediaTranscribeResponse
            Returns either transcription results, or a request_id when using a callback.

        Examples
        --------
        from deepgram import DeepgramClient

        client = DeepgramClient(
            api_key="YOUR_API_KEY",
        )
        client.listen.v1.media.transcribe_url(
            url="https://dpgr.am/spacewalk.wav",
        )
        """
        _response = self._raw_client.transcribe_url(
            url=url,
            callback=callback,
            callback_method=callback_method,
            extra=extra,
            sentiment=sentiment,
            summarize=summarize,
            tag=tag,
            topics=topics,
            custom_topic=custom_topic,
            custom_topic_mode=custom_topic_mode,
            intents=intents,
            custom_intent=custom_intent,
            custom_intent_mode=custom_intent_mode,
            detect_entities=detect_entities,
            detect_language=detect_language,
            diarize=diarize,
            dictation=dictation,
            encoding=encoding,
            filler_words=filler_words,
            keyterm=keyterm,
            keywords=keywords,
            language=language,
            measurements=measurements,
            model=model,
            multichannel=multichannel,
            numerals=numerals,
            paragraphs=paragraphs,
            profanity_filter=profanity_filter,
            punctuate=punctuate,
            redact=redact,
            replace=replace,
            search=search,
            smart_format=smart_format,
            utterances=utterances,
            utt_split=utt_split,
            version=version,
            mip_opt_out=mip_opt_out,
            request_options=request_options,
        )
        return _response.data

    def transcribe_file(
        self,
        *,
        request: typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]],
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> MediaTranscribeResponse:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        request : typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]]

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        MediaTranscribeResponse
            Returns either transcription results, or a request_id when using a callback.

        Examples
        --------
        from deepgram import DeepgramClient

        client = DeepgramClient(
            api_key="YOUR_API_KEY",
        )
        client.listen.v1.media.transcribe_file()
        """
        _response = self._raw_client.transcribe_file(
            request=request,
            callback=callback,
            callback_method=callback_method,
            extra=extra,
            sentiment=sentiment,
            summarize=summarize,
            tag=tag,
            topics=topics,
            custom_topic=custom_topic,
            custom_topic_mode=custom_topic_mode,
            intents=intents,
            custom_intent=custom_intent,
            custom_intent_mode=custom_intent_mode,
            detect_entities=detect_entities,
            detect_language=detect_language,
            diarize=diarize,
            dictation=dictation,
            encoding=encoding,
            filler_words=filler_words,
            keyterm=keyterm,
            keywords=keywords,
            language=language,
            measurements=measurements,
            model=model,
            multichannel=multichannel,
            numerals=numerals,
            paragraphs=paragraphs,
            profanity_filter=profanity_filter,
            punctuate=punctuate,
            redact=redact,
            replace=replace,
            search=search,
            smart_format=smart_format,
            utterances=utterances,
            utt_split=utt_split,
            version=version,
            mip_opt_out=mip_opt_out,
            request_options=request_options,
        )
        return _response.data


class AsyncMediaClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawMediaClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawMediaClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawMediaClient
        """
        return self._raw_client

    async def transcribe_url(
        self,
        *,
        url: str,
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> MediaTranscribeResponse:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        url : str

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        MediaTranscribeResponse
            Returns either transcription results, or a request_id when using a callback.

        Examples
        --------
        import asyncio

        from deepgram import AsyncDeepgramClient

        client = AsyncDeepgramClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.listen.v1.media.transcribe_url(
                url="https://dpgr.am/spacewalk.wav",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.transcribe_url(
            url=url,
            callback=callback,
            callback_method=callback_method,
            extra=extra,
            sentiment=sentiment,
            summarize=summarize,
            tag=tag,
            topics=topics,
            custom_topic=custom_topic,
            custom_topic_mode=custom_topic_mode,
            intents=intents,
            custom_intent=custom_intent,
            custom_intent_mode=custom_intent_mode,
            detect_entities=detect_entities,
            detect_language=detect_language,
            diarize=diarize,
            dictation=dictation,
            encoding=encoding,
            filler_words=filler_words,
            keyterm=keyterm,
            keywords=keywords,
            language=language,
            measurements=measurements,
            model=model,
            multichannel=multichannel,
            numerals=numerals,
            paragraphs=paragraphs,
            profanity_filter=profanity_filter,
            punctuate=punctuate,
            redact=redact,
            replace=replace,
            search=search,
            smart_format=smart_format,
            utterances=utterances,
            utt_split=utt_split,
            version=version,
            mip_opt_out=mip_opt_out,
            request_options=request_options,
        )
        return _response.data

    async def transcribe_file(
        self,
        *,
        request: typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]],
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> MediaTranscribeResponse:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        request : typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]]

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        MediaTranscribeResponse
            Returns either transcription results, or a request_id when using a callback.

        Examples
        --------
        import asyncio

        from deepgram import AsyncDeepgramClient

        client = AsyncDeepgramClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.listen.v1.media.transcribe_file()


        asyncio.run(main())
        """
        _response = await self._raw_client.transcribe_file(
            request=request,
            callback=callback,
            callback_method=callback_method,
            extra=extra,
            sentiment=sentiment,
            summarize=summarize,
            tag=tag,
            topics=topics,
            custom_topic=custom_topic,
            custom_topic_mode=custom_topic_mode,
            intents=intents,
            custom_intent=custom_intent,
            custom_intent_mode=custom_intent_mode,
            detect_entities=detect_entities,
            detect_language=detect_language,
            diarize=diarize,
            dictation=dictation,
            encoding=encoding,
            filler_words=filler_words,
            keyterm=keyterm,
            keywords=keywords,
            language=language,
            measurements=measurements,
            model=model,
            multichannel=multichannel,
            numerals=numerals,
            paragraphs=paragraphs,
            profanity_filter=profanity_filter,
            punctuate=punctuate,
            redact=redact,
            replace=replace,
            search=search,
            smart_format=smart_format,
            utterances=utterances,
            utt_split=utt_split,
            version=version,
            mip_opt_out=mip_opt_out,
            request_options=request_options,
        )
        return _response.data
