# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ....core.api_error import ApiError
from ....core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ....core.http_response import AsyncHttpResponse, HttpResponse
from ....core.request_options import RequestOptions
from ....core.unchecked_base_model import construct_type
from ....errors.bad_request_error import BadRequestError
from .types.media_transcribe_request_callback_method import MediaTranscribeRequestCallbackMethod
from .types.media_transcribe_request_custom_intent_mode import MediaTranscribeRequestCustomIntentMode
from .types.media_transcribe_request_custom_topic_mode import MediaTranscribeRequestCustomTopicMode
from .types.media_transcribe_request_encoding import MediaTranscribeRequestEncoding
from .types.media_transcribe_request_model import MediaTranscribeRequestModel
from .types.media_transcribe_request_summarize import MediaTranscribeRequestSummarize
from .types.media_transcribe_request_version import MediaTranscribeRequestVersion
from .types.media_transcribe_response import MediaTranscribeResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawMediaClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def transcribe_url(
        self,
        *,
        url: str,
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[MediaTranscribeResponse]:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        url : str

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[MediaTranscribeResponse]
            Returns either transcription results, or a request_id when using a callback.
        """
        _response = self._client_wrapper.httpx_client.request(
            "v1/listen",
            base_url=self._client_wrapper.get_environment().base,
            method="POST",
            params={
                "callback": callback,
                "callback_method": callback_method,
                "extra": extra,
                "sentiment": sentiment,
                "summarize": summarize,
                "tag": tag,
                "topics": topics,
                "custom_topic": custom_topic,
                "custom_topic_mode": custom_topic_mode,
                "intents": intents,
                "custom_intent": custom_intent,
                "custom_intent_mode": custom_intent_mode,
                "detect_entities": detect_entities,
                "detect_language": detect_language,
                "diarize": diarize,
                "dictation": dictation,
                "encoding": encoding,
                "filler_words": filler_words,
                "keyterm": keyterm,
                "keywords": keywords,
                "language": language,
                "measurements": measurements,
                "model": model,
                "multichannel": multichannel,
                "numerals": numerals,
                "paragraphs": paragraphs,
                "profanity_filter": profanity_filter,
                "punctuate": punctuate,
                "redact": redact,
                "replace": replace,
                "search": search,
                "smart_format": smart_format,
                "utterances": utterances,
                "utt_split": utt_split,
                "version": version,
                "mip_opt_out": mip_opt_out,
            },
            json={
                "url": url,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    MediaTranscribeResponse,
                    construct_type(
                        type_=MediaTranscribeResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 400:
                raise BadRequestError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        typing.Any,
                        construct_type(
                            type_=typing.Any,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def transcribe_file(
        self,
        *,
        request: typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]],
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[MediaTranscribeResponse]:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        request : typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]]

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[MediaTranscribeResponse]
            Returns either transcription results, or a request_id when using a callback.
        """
        _response = self._client_wrapper.httpx_client.request(
            "v1/listen",
            base_url=self._client_wrapper.get_environment().base,
            method="POST",
            params={
                "callback": callback,
                "callback_method": callback_method,
                "extra": extra,
                "sentiment": sentiment,
                "summarize": summarize,
                "tag": tag,
                "topics": topics,
                "custom_topic": custom_topic,
                "custom_topic_mode": custom_topic_mode,
                "intents": intents,
                "custom_intent": custom_intent,
                "custom_intent_mode": custom_intent_mode,
                "detect_entities": detect_entities,
                "detect_language": detect_language,
                "diarize": diarize,
                "dictation": dictation,
                "encoding": encoding,
                "filler_words": filler_words,
                "keyterm": keyterm,
                "keywords": keywords,
                "language": language,
                "measurements": measurements,
                "model": model,
                "multichannel": multichannel,
                "numerals": numerals,
                "paragraphs": paragraphs,
                "profanity_filter": profanity_filter,
                "punctuate": punctuate,
                "redact": redact,
                "replace": replace,
                "search": search,
                "smart_format": smart_format,
                "utterances": utterances,
                "utt_split": utt_split,
                "version": version,
                "mip_opt_out": mip_opt_out,
            },
            content=request,
            headers={
                "content-type": "application/octet-stream",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    MediaTranscribeResponse,
                    construct_type(
                        type_=MediaTranscribeResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 400:
                raise BadRequestError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        typing.Any,
                        construct_type(
                            type_=typing.Any,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)


class AsyncRawMediaClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def transcribe_url(
        self,
        *,
        url: str,
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[MediaTranscribeResponse]:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        url : str

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[MediaTranscribeResponse]
            Returns either transcription results, or a request_id when using a callback.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v1/listen",
            base_url=self._client_wrapper.get_environment().base,
            method="POST",
            params={
                "callback": callback,
                "callback_method": callback_method,
                "extra": extra,
                "sentiment": sentiment,
                "summarize": summarize,
                "tag": tag,
                "topics": topics,
                "custom_topic": custom_topic,
                "custom_topic_mode": custom_topic_mode,
                "intents": intents,
                "custom_intent": custom_intent,
                "custom_intent_mode": custom_intent_mode,
                "detect_entities": detect_entities,
                "detect_language": detect_language,
                "diarize": diarize,
                "dictation": dictation,
                "encoding": encoding,
                "filler_words": filler_words,
                "keyterm": keyterm,
                "keywords": keywords,
                "language": language,
                "measurements": measurements,
                "model": model,
                "multichannel": multichannel,
                "numerals": numerals,
                "paragraphs": paragraphs,
                "profanity_filter": profanity_filter,
                "punctuate": punctuate,
                "redact": redact,
                "replace": replace,
                "search": search,
                "smart_format": smart_format,
                "utterances": utterances,
                "utt_split": utt_split,
                "version": version,
                "mip_opt_out": mip_opt_out,
            },
            json={
                "url": url,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    MediaTranscribeResponse,
                    construct_type(
                        type_=MediaTranscribeResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 400:
                raise BadRequestError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        typing.Any,
                        construct_type(
                            type_=typing.Any,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def transcribe_file(
        self,
        *,
        request: typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]],
        callback: typing.Optional[str] = None,
        callback_method: typing.Optional[MediaTranscribeRequestCallbackMethod] = None,
        extra: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        sentiment: typing.Optional[bool] = None,
        summarize: typing.Optional[MediaTranscribeRequestSummarize] = None,
        tag: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        topics: typing.Optional[bool] = None,
        custom_topic: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_topic_mode: typing.Optional[MediaTranscribeRequestCustomTopicMode] = None,
        intents: typing.Optional[bool] = None,
        custom_intent: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        custom_intent_mode: typing.Optional[MediaTranscribeRequestCustomIntentMode] = None,
        detect_entities: typing.Optional[bool] = None,
        detect_language: typing.Optional[bool] = None,
        diarize: typing.Optional[bool] = None,
        dictation: typing.Optional[bool] = None,
        encoding: typing.Optional[MediaTranscribeRequestEncoding] = None,
        filler_words: typing.Optional[bool] = None,
        keyterm: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        keywords: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        language: typing.Optional[str] = None,
        measurements: typing.Optional[bool] = None,
        model: typing.Optional[MediaTranscribeRequestModel] = None,
        multichannel: typing.Optional[bool] = None,
        numerals: typing.Optional[bool] = None,
        paragraphs: typing.Optional[bool] = None,
        profanity_filter: typing.Optional[bool] = None,
        punctuate: typing.Optional[bool] = None,
        redact: typing.Optional[str] = None,
        replace: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        search: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        smart_format: typing.Optional[bool] = None,
        utterances: typing.Optional[bool] = None,
        utt_split: typing.Optional[float] = None,
        version: typing.Optional[MediaTranscribeRequestVersion] = None,
        mip_opt_out: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[MediaTranscribeResponse]:
        """
        Transcribe audio and video using Deepgram's speech-to-text REST API

        Parameters
        ----------
        request : typing.Union[bytes, typing.Iterator[bytes], typing.AsyncIterator[bytes]]

        callback : typing.Optional[str]
            URL to which we'll make the callback request

        callback_method : typing.Optional[MediaTranscribeRequestCallbackMethod]
            HTTP method by which the callback request will be made

        extra : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Arbitrary key-value pairs that are attached to the API response for usage in downstream processing

        sentiment : typing.Optional[bool]
            Recognizes the sentiment throughout a transcript or text

        summarize : typing.Optional[MediaTranscribeRequestSummarize]
            Summarize content. For Listen API, supports string version option. For Read API, accepts boolean only.

        tag : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Label your requests for the purpose of identification during usage reporting

        topics : typing.Optional[bool]
            Detect topics throughout a transcript or text

        custom_topic : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom topics you want the model to detect within your input audio or text if present Submit up to `100`.

        custom_topic_mode : typing.Optional[MediaTranscribeRequestCustomTopicMode]
            Sets how the model will interpret strings submitted to the `custom_topic` param. When `strict`, the model will only return topics submitted using the `custom_topic` param. When `extended`, the model will return its own detected topics in addition to those submitted using the `custom_topic` param

        intents : typing.Optional[bool]
            Recognizes speaker intent throughout a transcript or text

        custom_intent : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Custom intents you want the model to detect within your input audio if present

        custom_intent_mode : typing.Optional[MediaTranscribeRequestCustomIntentMode]
            Sets how the model will interpret intents submitted to the `custom_intent` param. When `strict`, the model will only return intents submitted using the `custom_intent` param. When `extended`, the model will return its own detected intents in the `custom_intent` param.

        detect_entities : typing.Optional[bool]
            Identifies and extracts key entities from content in submitted audio

        detect_language : typing.Optional[bool]
            Identifies the dominant language spoken in submitted audio

        diarize : typing.Optional[bool]
            Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0

        dictation : typing.Optional[bool]
            Dictation mode for controlling formatting with dictated speech

        encoding : typing.Optional[MediaTranscribeRequestEncoding]
            Specify the expected encoding of your submitted audio

        filler_words : typing.Optional[bool]
            Filler Words can help transcribe interruptions in your audio, like "uh" and "um"

        keyterm : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Key term prompting can boost or suppress specialized terminology and brands. Only compatible with Nova-3

        keywords : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Keywords can boost or suppress specialized terminology and brands

        language : typing.Optional[str]
            The [BCP-47 language tag](https://tools.ietf.org/html/bcp47) that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available

        measurements : typing.Optional[bool]
            Spoken measurements will be converted to their corresponding abbreviations

        model : typing.Optional[MediaTranscribeRequestModel]
            AI model used to process submitted audio

        multichannel : typing.Optional[bool]
            Transcribe each audio channel independently

        numerals : typing.Optional[bool]
            Numerals converts numbers from written format to numerical format

        paragraphs : typing.Optional[bool]
            Splits audio into paragraphs to improve transcript readability

        profanity_filter : typing.Optional[bool]
            Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely

        punctuate : typing.Optional[bool]
            Add punctuation and capitalization to the transcript

        redact : typing.Optional[str]
            Redaction removes sensitive information from your transcripts

        replace : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio and replaces them

        search : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Search for terms or phrases in submitted audio

        smart_format : typing.Optional[bool]
            Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability

        utterances : typing.Optional[bool]
            Segments speech into meaningful semantic units

        utt_split : typing.Optional[float]
            Seconds to wait before detecting a pause between words in submitted audio

        version : typing.Optional[MediaTranscribeRequestVersion]
            Version of an AI model to use

        mip_opt_out : typing.Optional[bool]
            Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[MediaTranscribeResponse]
            Returns either transcription results, or a request_id when using a callback.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v1/listen",
            base_url=self._client_wrapper.get_environment().base,
            method="POST",
            params={
                "callback": callback,
                "callback_method": callback_method,
                "extra": extra,
                "sentiment": sentiment,
                "summarize": summarize,
                "tag": tag,
                "topics": topics,
                "custom_topic": custom_topic,
                "custom_topic_mode": custom_topic_mode,
                "intents": intents,
                "custom_intent": custom_intent,
                "custom_intent_mode": custom_intent_mode,
                "detect_entities": detect_entities,
                "detect_language": detect_language,
                "diarize": diarize,
                "dictation": dictation,
                "encoding": encoding,
                "filler_words": filler_words,
                "keyterm": keyterm,
                "keywords": keywords,
                "language": language,
                "measurements": measurements,
                "model": model,
                "multichannel": multichannel,
                "numerals": numerals,
                "paragraphs": paragraphs,
                "profanity_filter": profanity_filter,
                "punctuate": punctuate,
                "redact": redact,
                "replace": replace,
                "search": search,
                "smart_format": smart_format,
                "utterances": utterances,
                "utt_split": utt_split,
                "version": version,
                "mip_opt_out": mip_opt_out,
            },
            content=request,
            headers={
                "content-type": "application/octet-stream",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    MediaTranscribeResponse,
                    construct_type(
                        type_=MediaTranscribeResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 400:
                raise BadRequestError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        typing.Any,
                        construct_type(
                            type_=typing.Any,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)
